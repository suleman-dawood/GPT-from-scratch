# GPT-from-scratch
Creating a basic GPT model from scratch

This code implements a transformer-based language model inspired by modern architectures like GPT. Its goal is to train a neural network to predict and generate text based on the works of Friedrich Nietzsche. By learning the relationships between characters and their contexts, the model can produce new, contextually relevant sequences of text. For now it only has the encoder so it somply tries to generate text 
